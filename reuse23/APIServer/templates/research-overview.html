{% extends 'base.html' %}

{% block title %}Research Overview{% endblock %}

{% block body %}
<style>
    h2 {
        padding: 0 20px;
    }

    p {
        margin: 20px 20px;
    }

    li {
        margin: 5px 0;
    }
</style>
    <h1 style="text-align: center; padding: 20px 0;">Research Overview</h1>
    <div class="box">
        <div class="clearfix">
            <img style="float: right; width: 600px; height: 300px;" src="https://upload.wikimedia.org/wikipedia/commons/1/18/Dog_Breeds.jpg" alt="dog">
            <h2 style="text-align: left;">Key Terms & Definitions</h2>
            <ul>
                <li><b>Engine:</b> a software (code) that interprets and compiles binary code (such as Wasm) using its various tiers; also called a virtual machine (VM)</li>
                <li><b>Tier:</b> an interpreter or compiler in an engine that affect an engine's performance when enabled/disabled</li>
                <li><b>Interpreter:</b> a tier of an engine that inspects each byte of code and decides what to do</li>
                <li><b>Compiler:</b> a tier of an engine that compiles code (often machine code) from the bytecode; multiple compilers are used to produce high quality code quickly</li>
                <li><b>Benchmarks:</b> a very small portion of the engine program</li>
                <li><b>Suites:</b> a collection of related programs in a benchmark</li>
                <li><b>Line Item:</b> a simple portion of code within a suite</li>
            </ul>
        </div>
    </div>

    <div class="box">
        <div class="clearfix">
            <h2>Introduction</h2> 
            <p style="text-align: left;">The purpose of this project is to study Wasm engine performance.
	      Building off of previous research on the Wizard engine,
              this project compares engine performance across several different metrics: <i>total time</i>, <i>main time</i>, and <i>set up time</i>.
	      Total time represents an entire process execution, from engine startup to program completion.
	      Main time represents the time it takes for the program to execute its "main" function.
	      Set up time repreets program-related startup costs such as loading, parsing, validating, and potentially compiling Wasm modules, up to the first bytecode executed.
              In these experiments, we test three benchmark suites with varying numbers of line items: PolyBench/C with 29 line items, Ostrich with 11 line items, and Libsodium with 39; 79 wasm files in total.
              The tested engines were built from source code retrieved from their respective GitHubs. 
            </p>
            <img style="float: left; width: 600px; height: 300px;" src="https://upload.wikimedia.org/wikipedia/commons/1/18/Dog_Breeds.jpg" alt="dog">
            <h2>Methods</h2>
            <p style="text-align: left;">Data was collected on a small number of lab machines, and the database schema is designed to incorporate data from multiple diverse machines in the future.
	      Every experiment includes multiple runs, stored as raw data, and various statistical measures and summaries.
	      Engine startup time and setup time are measured using the "empty" program and benchmark line items altered to exit early.
	      Main time is calculated by subtracting the time taken to execute a full benchmark versus the its exit-early alteration.
	      We did not run experiments for startup time and used values from previous research.
	      In order to visualize our data, we used a PostgreSQL database connected to an API server to generate the graphs on this website. 
            </p>
        </div> 
    </div> 

    <div class="box">
        <div class="clearfix">
            <h2>Results & Further Extensions</h2>
            <p style="text-align: left;">Our results shed light on the tradeoff between the setup time (which may include compilation time) and execution time over various execution tiers for Wasm.
	      We are always tweaking methodology.
Due to measurement error, 
                there are some instances in which the zero file's average is greater than the nonzero file's average.
 As a result, for very short-running benchmarks, the main time can be overwhelmed by the statistical variance of engine startup.
 The results from these experiment may vary when executed on a different machine.
            </p>
        </div>
    </div>
{% endblock %}
